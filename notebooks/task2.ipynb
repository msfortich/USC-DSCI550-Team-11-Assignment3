{"cells":[{"cell_type":"markdown","metadata":{"id":"4BVRPYui2QJ0"},"source":["Task 2 TSV to JSON Scripts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DTSIyUTB2QJ1"},"outputs":[],"source":["import pandas as pd\n","import json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO_QdHWw2QJ2","outputId":"16767cd1-c4a7-49a1-ab53-5f7ef81f0803"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/nw/tjpcs72n3p50mg20k2vgl11w0000gn/T/ipykernel_2127/2084665016.py:2: DtypeWarning: Columns (135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,171,172,173,174,175,176,177,178,179,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv('../data/tsvOriginal/BFROFinal.tsv', sep='\\t')\n"]}],"source":["#code to remove redundant columns\n","df = pd.read_csv('../data/tsvOriginal/BFROFinal.tsv', sep='\\t')\n","filtered_columns = [col for col in df.columns if 'Report_Entity' not in col and 'Observed_Entity' not in col and 'Also_Noticed_Entity' not in col]\n","data = df[filtered_columns]\n","# data.to_csv('new_data.tsv', sep='\\t', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bQLSDmg02QJ3"},"outputs":[],"source":["#data for grouped bar chart\n","class_agg = data.groupby([\"State\", \"Class\"]).size().reset_index(name=\"count\")\n","class_agg.to_json(\"../data/class_agg.json\", orient=\"records\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xGws52f42QJ3"},"outputs":[],"source":["#data for sightings and witnesses over time\n","sightings = data.groupby('yr').size().reset_index(name='TotalSightings')\n","avg_witness_count = data.groupby('yr')['WitnessCount'].mean().reset_index()\n","over_time = pd.merge(sightings, avg_witness_count, on='yr', how='left')\n","over_time.rename(columns={'WitnessCount': 'AvgWitnessCount'}, inplace=True)\n","over_time = over_time.sort_values(by = 'TotalSightings', ascending = False)\n","over_time = over_time.head(30)\n","over_time = over_time.sort_values(by = 'yr', ascending = True)\n","over_time = over_time.to_json(orient='records')\n","\n","with open('../data/over_time.json', 'w') as file:\n","    file.write(over_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHLKXAHT2QJ3"},"outputs":[],"source":["# getting numebr of sightings per State\n","StateAndSighting = data['State'].value_counts().reset_index()\n","StateAndSighting.columns = ['State', 'Sightings']\n","stateandsightingsjson = StateAndSighting.to_json(orient='records')\n","with open('../data/sighting.json', 'w') as f:\n","    f.write(stateandsightingsjson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bu1uW3MW2QJ3"},"outputs":[],"source":["# Got lat/lng for each state and combined it with the sightings json\n","#downloaded lat/lng from https://gist.github.com/meiqimichelle/7727723\n","sightings_data = pd.read_json('../data/sighting.json')\n","geo_data = pd.read_json('../data/USstates_avg_latLong.json')\n","combined_data = pd.merge(sightings_data, geo_data, how='left', left_on='State', right_on='state')\n","combined_data.head()\n","stateandsightingsgeojson = combined_data.to_json(orient='records')\n","with open('../data/SightingCountandGeo.json', 'w') as f:\n","    f.write(stateandsightingsgeojson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhpRoIX82QJ4","outputId":"6e0d1b6a-7965-427a-e61a-5e0f4d3a3f8b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/var/folders/nw/tjpcs72n3p50mg20k2vgl11w0000gn/T/ipykernel_2127/2542045105.py:3: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  data['BigFoot Hotspot'] = data.apply(lambda x: (x['State'], x['County']) in [tuple(x) for x in top10locations.values], axis=1)\n"]}],"source":["# getting bigfoot hotspot info\n","top10locations = data.groupby(['State', 'County']).size().nlargest(10).reset_index()[['State', 'County']]\n","data['BigFoot Hotspot'] = data.apply(lambda x: (x['State'], x['County']) in [tuple(x) for x in top10locations.values], axis=1)\n","bigfoothotspotsjson = data[['State', 'County', 'BigFoot Hotspot']].to_json(orient='records')\n","bigfoot_hotspots_file = '../data/bigfoothotspots.json'\n","with open(bigfoot_hotspots_file, 'w') as f:\n","    f.write(bigfoothotspotsjson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E1In7QH12QJ4"},"outputs":[],"source":["#getting witness counts per state by season\n","Witnessdf = data.dropna(subset=['State', 'Season', 'WitnessCount'])\n","Witnessdf = Witnessdf[(Witnessdf['Season'] != 'Unknown') & (Witnessdf['WitnessCount'] > 1)]\n","WitnessJson = Witnessdf[['State', 'Season', 'Class', 'WitnessCount']].to_json(orient='records')\n","WitnessJsonFile = '../data/Witnesses.json'\n","with open(WitnessJsonFile, 'w') as f:\n","    f.write(WitnessJson)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zCFYiPMo2QJ4","outputId":"0c0b01f9-c9e2-4957-97e3-dc001ea1b23a"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Class</th>\n","      <th>mth</th>\n","      <th>closestPark</th>\n","      <th>Severity</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>22</th>\n","      <td>Class A</td>\n","      <td>September</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>Class A</td>\n","      <td>January</td>\n","      <td>tuskegee institute nhs</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>Class B</td>\n","      <td>September</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>Class A</td>\n","      <td>June</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>Class B</td>\n","      <td>September</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>Class C</td>\n","      <td>June</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>Class A</td>\n","      <td>June</td>\n","      <td>little river canyon npres</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>Class A</td>\n","      <td>July</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>Class B</td>\n","      <td>October</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>Class B</td>\n","      <td>September</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>Class A</td>\n","      <td>December</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>Class B</td>\n","      <td>November</td>\n","      <td>horseshoe bend nmp</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>Class B</td>\n","      <td>January</td>\n","      <td>horseshoe bend nmp</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>Class A</td>\n","      <td>January</td>\n","      <td>tuskegee institute nhs</td>\n","      <td>UNK</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>Class B</td>\n","      <td>November</td>\n","      <td>shiloh nmp</td>\n","      <td>Moderate</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>Class A</td>\n","      <td>January</td>\n","      <td>shiloh nmp</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>Class A</td>\n","      <td>September</td>\n","      <td>shiloh nmp</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>Class A</td>\n","      <td>August</td>\n","      <td>gulf islands ns</td>\n","      <td>Moderate</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>Class A</td>\n","      <td>February</td>\n","      <td>gulf islands ns</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>51</th>\n","      <td>Class B</td>\n","      <td>March</td>\n","      <td>gulf islands ns</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>Class A</td>\n","      <td>October</td>\n","      <td>gulf islands ns</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>Class A</td>\n","      <td>June</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>Class A</td>\n","      <td>January</td>\n","      <td>little river canyon npres</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>55</th>\n","      <td>Class B</td>\n","      <td>April</td>\n","      <td>tuskegee institute nhs</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>56</th>\n","      <td>Class B</td>\n","      <td>April</td>\n","      <td>tuskegee institute nhs</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>57</th>\n","      <td>Class A</td>\n","      <td>July</td>\n","      <td>tuskegee institute nhs</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>Class A</td>\n","      <td>September</td>\n","      <td>gulf islands ns</td>\n","      <td>Light</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>Class B</td>\n","      <td>December</td>\n","      <td>brices cross roads</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>63</th>\n","      <td>Class A</td>\n","      <td>December</td>\n","      <td>brices cross roads</td>\n","      <td>Severe</td>\n","    </tr>\n","    <tr>\n","      <th>64</th>\n","      <td>Class A</td>\n","      <td>June</td>\n","      <td>jimmy carter nhp</td>\n","      <td>Light</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      Class        mth                closestPark  Severity\n","22  Class A  September            gulf islands ns     Light\n","23  Class A    January     tuskegee institute nhs     Light\n","25  Class B  September  little river canyon npres     Light\n","26  Class A       June  little river canyon npres     Light\n","27  Class B  September  little river canyon npres     Light\n","28  Class C       June  little river canyon npres     Light\n","30  Class A       June  little river canyon npres    Severe\n","36  Class A       July            gulf islands ns     Light\n","37  Class B    October            gulf islands ns     Light\n","38  Class B  September            gulf islands ns     Light\n","39  Class A   December            gulf islands ns     Light\n","40  Class B   November         horseshoe bend nmp     Light\n","41  Class B    January         horseshoe bend nmp     Light\n","44  Class A    January     tuskegee institute nhs       UNK\n","45  Class B   November                 shiloh nmp  Moderate\n","46  Class A    January                 shiloh nmp     Light\n","47  Class A  September                 shiloh nmp     Light\n","48  Class A     August            gulf islands ns  Moderate\n","49  Class A   February            gulf islands ns    Severe\n","51  Class B      March            gulf islands ns    Severe\n","52  Class A    October            gulf islands ns    Severe\n","53  Class A       June  little river canyon npres     Light\n","54  Class A    January  little river canyon npres     Light\n","55  Class B      April     tuskegee institute nhs     Light\n","56  Class B      April     tuskegee institute nhs     Light\n","57  Class A       July     tuskegee institute nhs     Light\n","59  Class A  September            gulf islands ns     Light\n","62  Class B   December         brices cross roads    Severe\n","63  Class A   December         brices cross roads    Severe\n","64  Class A       June           jimmy carter nhp     Light"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#grouping by class, month, closest park, and severity\n","df = data[['Class', 'mth', 'closestPark', 'Severity']]\n","df = df[~df['Class'].isnull()]\n","df = df[~df['closestPark'].isnull()]\n","df = df[~df['Severity'].isnull()]\n","df.head(30)\n","#count is 4059)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QitU88EN2QJ4"},"outputs":[],"source":["# Group the data by the features\n","grouped_data = df.groupby([\"closestPark\", \"Class\", \"mth\", \"Severity\"]).size().reset_index(name=\"count\")\n","# Define a function to convert DataFrame to hierarchical JSON\n","def df_to_json(df):\n","    data = {\"name\": \"root\", \"children\": []}\n","    for _, row in df.iterrows():\n","        current_node = data\n","        for feature in [\"closestPark\", \"Class\", \"mth\", \"Severity\"]:\n","            value = row[feature]\n","            children = current_node.get(\"children\", [])\n","            child = next((c for c in children if c[\"name\"] == value), None)\n","            if child is None:\n","                child = {\"name\": value, \"children\": []}\n","                children.append(child)\n","            current_node[\"children\"] = children\n","            current_node = child\n","        current_node[\"value\"] = row[\"count\"]  # Add count as value\n","    return data\n","\n","# Convert DataFrame to hierarchical JSON\n","hierarchical_json = df_to_json(grouped_data)\n","\n","# Save the JSON to a file\n","with open(\"../data/classMonthParkWeather.json\", \"w\") as json_file:\n","    json.dump(hierarchical_json, json_file, indent=4)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}